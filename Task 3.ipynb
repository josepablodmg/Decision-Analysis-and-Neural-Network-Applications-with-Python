{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6714c553",
   "metadata": {},
   "source": [
    "### **Load and Preprocess the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e1a76c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load the MNIST dataset\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "\n",
    "def to_categorical(labels, num_classes):\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "y_train_cat = to_categorical(y_train, num_classes=10)\n",
    "y_test_cat = to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80864a28",
   "metadata": {},
   "source": [
    "### **What we did:**\n",
    "    \n",
    "**Loading the MNIST Dataset:**\n",
    "    \n",
    "The MNIST dataset is widely used for benchmarking image classification algorithms.\n",
    "We used mnist.load_data() to load the dataset into training and testing sets.\n",
    "\n",
    "**Normalizing the Data:**\n",
    "\n",
    "Normalizing the data helps in faster convergence of neural networks by scaling the pixel values to [0, 1].\n",
    "We divided each pixel value in X_train and X_test by 255.0.\n",
    "\n",
    "**Flattening the Images:**\n",
    "\n",
    "Neural networks, especially fully connected layers, require input as a 1D vector rather than a 2D image.\n",
    "We reshaped X_train and X_test from (28, 28) to (784,).\n",
    "\n",
    "**Converting Labels to Categorical One-Hot Encoding:**\n",
    "\n",
    "One-hot encoding is necessary for multi-class classification as it converts class labels into a format suitable for the neural networkâ€™s output layer.\n",
    "We created a to_categorical function to convert labels to a binary matrix format, then applied this function to y_train and y_test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b6ae4f",
   "metadata": {},
   "source": [
    "### **What happens if we change the variables?**\n",
    "\n",
    "**Normalization:** Changing the normalization factor can impact how quickly and effectively the model learns.\n",
    "\n",
    "**Flattening:** Incorrect flattening would result in input size mismatch.\n",
    "\n",
    "**One-hot Encoding:** Not using one-hot encoding would prevent proper multi-class classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337d42e2",
   "metadata": {},
   "source": [
    "### Part A: Why are 784 layers appropriate for the input layer? \n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**784 Input Features:** Each MNIST image is 28x28 pixels, which equals 784 pixels when flattened.\n",
    "\n",
    "**Input Layer Configuration:** Neural networks require input data to be in a 1D vector format for fully connected layers.\n",
    "\n",
    "### What happens if we change the input layer size?\n",
    "\n",
    "**Less than 784:** The network would lose some pixel information, leading to worse performance.\n",
    "\n",
    "**More than 784:** This would be unnecessary for the MNIST dataset and could lead to incorrect input dimensions.\n",
    "\n",
    "### Part B: Why are 10 layers appropriate for the output layer? Are there other options? \n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**10 Output Classes:** The MNIST dataset has 10 classes (digits 0 through 9).\n",
    "\n",
    "**Output Layer Configuration:** Using 10 output nodes corresponds to the 10 possible digit classes, with softmax activation to output probabilities.\n",
    "\n",
    "### What happens if we change the output layer size?\n",
    "\n",
    "**Less than 10:** The network would not be able to classify all 10 digits.\n",
    "\n",
    "**More than 10:** This would be unnecessary and would not match the number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2570d36",
   "metadata": {},
   "source": [
    "### **Define the Neural Network**\n",
    "\n",
    "Define a function create_model that takes hidden_layer_size and learning_rate as parameters and returns a compiled Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df681a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN:\n",
    "    def __init__(self, input_dim, hidden_layer_size, output_dim, learning_rate):\n",
    "        self.weights1 = np.random.randn(input_dim, hidden_layer_size) * 0.01 # Weights and bias from input layer to hidden layer.\n",
    "        self.bias1 = np.zeros(hidden_layer_size)\n",
    "        self.weights2 = np.random.randn(hidden_layer_size, output_dim) * 0.01 # Weights and bias from hidden layer to output layer.\n",
    "        self.bias2 = np.zeros(output_dim)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.z1 = np.dot(x, self.weights1) + self.bias1\n",
    "        self.a1 = self.relu(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.weights2) + self.bias2\n",
    "        self.a2 = self.softmax(self.z2)\n",
    "        return self.a2\n",
    "\n",
    "    def backward(self, x, y, output):\n",
    "        m = y.shape[0]\n",
    "        dz2 = output - y #Calculating error\n",
    "        dw2 = np.dot(self.a1.T, dz2) / m #Calculating gradient of the weights by doing matrix multiplication\n",
    "        db2 = np.sum(dz2, axis=0) / m #Calculating gradient of the biases by summing the error in the output layer & dividing by number of rows.\n",
    "\n",
    "        dz1 = np.dot(dz2, self.weights2.T) * (self.a1 > 0) #Calculating error in the hidden layer\n",
    "        dw1 = np.dot(x.T, dz1) / m #Calculating gradient of the weights\n",
    "        db1 = np.sum(dz1, axis=0) / m #Calculating gradient of the biases\n",
    "\n",
    "        self.weights2 -= self.learning_rate * dw2 #Updating weights subtracting the gradient multiplied by the learning rate.\n",
    "        self.bias2 -= self.learning_rate * db2 #Updating biases\n",
    "        self.weights1 -= self.learning_rate * dw1\n",
    "        self.bias1 -= self.learning_rate * db1\n",
    "\n",
    "    def compile(self):\n",
    "        pass  \n",
    "\n",
    "    def fit(self, x, y, epochs, batch_size): #performing mini-batch gradient descent\n",
    "        for epoch in range(epochs):\n",
    "            indices = np.arange(x.shape[0])\n",
    "            np.random.shuffle(indices) #random shuffle so that the model doesn't learn the order of the data and we can generalize better.\n",
    "            x = x[indices]\n",
    "            y = y[indices]\n",
    "\n",
    "            for i in range(0, x.shape[0], batch_size):\n",
    "                x_batch = x[i:i + batch_size]\n",
    "                y_batch = y[i:i + batch_size]\n",
    "                output = self.forward(x_batch)\n",
    "                self.backward(x_batch, y_batch, output)\n",
    "\n",
    "    def evaluate(self, x, y):\n",
    "        output = self.forward(x)\n",
    "        predictions = np.argmax(output, axis=1)\n",
    "        targets = np.argmax(y, axis=1)\n",
    "        accuracy = np.mean(predictions == targets)\n",
    "        return accuracy\n",
    "\n",
    "# Define a function to create the model\n",
    "\n",
    "def create_model(hidden_layer_size, learning_rate):\n",
    "    input_dim = 784\n",
    "    output_dim = 10\n",
    "    return SimpleNN(input_dim, hidden_layer_size, output_dim, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c13d15",
   "metadata": {},
   "source": [
    "### **What we did:**\n",
    "    \n",
    "**Initialization:** \n",
    "\n",
    "We initialize a neural network with one hidden layer, specifying input, hidden, and output dimensions, and setting small random weights and zero biases.\n",
    "\n",
    "**Activation Functions:** \n",
    "\n",
    "We use ReLU for the hidden layer and softmax for the output layer to handle non-linearity and produce probability distributions.\n",
    "\n",
    "**Forward Pass:** \n",
    "\n",
    "We compute activations by performing matrix multiplications and applying activation functions.\n",
    "\n",
    "**Backward Pass:** \n",
    "\n",
    "We calculate gradients and update weights and biases using gradient descent.\n",
    "\n",
    "**Training:** \n",
    "\n",
    "We shuffle the data, split it into mini-batches, and iteratively update the model parameters through forward and backward passes over multiple epochs.\n",
    "\n",
    "**Evaluation:**\n",
    "\n",
    "We assess model performance by calculating the accuracy on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696a718d",
   "metadata": {},
   "source": [
    "### What happens if we change the variables?\n",
    "\n",
    "**hidden_layer_size:** More hidden units can capture more complex patterns but may lead to overfitting.\n",
    "    \n",
    "**learning_rate:** Adjusting the learning rate affects training speed and convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28806e6",
   "metadata": {},
   "source": [
    "###  Part C: Run a backpropagation algorithm on a smaller subset\n",
    "\n",
    "Use a subset of the training data to find reasonable values for the hidden layer size and learning rate.\n",
    "\n",
    "Train and evaluate the model on the subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06f0a723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training subset:  (6000, 784)\n",
      "Epoch 1/10, Validation Accuracy: 0.6558333333333334\n",
      "Epoch 2/10, Validation Accuracy: 0.825\n",
      "Epoch 3/10, Validation Accuracy: 0.8808333333333334\n",
      "Epoch 4/10, Validation Accuracy: 0.8941666666666667\n",
      "Epoch 5/10, Validation Accuracy: 0.8966666666666666\n",
      "Epoch 6/10, Validation Accuracy: 0.9008333333333334\n",
      "Epoch 7/10, Validation Accuracy: 0.9133333333333333\n",
      "Epoch 8/10, Validation Accuracy: 0.9125\n",
      "Epoch 9/10, Validation Accuracy: 0.9166666666666666\n",
      "Epoch 10/10, Validation Accuracy: 0.9141666666666667\n",
      "Subset Training Accuracy: 0.9051\n"
     ]
    }
   ],
   "source": [
    "# We use a smaller subset for initial experiments\n",
    "\n",
    "np.random.seed(42)\n",
    "subset_indices = np.random.choice(X_train.shape[0], int(0.1 * X_train.shape[0]), replace=False) #Random 10% of the dataset.\n",
    "X_train_subset = X_train[subset_indices]\n",
    "y_train_subset = y_train_cat[subset_indices]\n",
    "\n",
    "# Experiment with initial hidden layer size and learning rate\n",
    "print(\"Shape of the training subset: \", X_train_subset.shape)\n",
    "initial_hidden_layer_size = 50\n",
    "initial_learning_rate = 0.17\n",
    "\n",
    "# Create the model\n",
    "\n",
    "model = create_model(initial_hidden_layer_size, initial_learning_rate)\n",
    "\n",
    "# Train the model\n",
    "\n",
    "def train_with_validation(model, X, y, epochs, batch_size, validation_split):\n",
    "    split_index = int((1 - validation_split) * X.shape[0])\n",
    "    X_train, X_val = X[:split_index], X[split_index:]\n",
    "    y_train, y_val = y[:split_index], y[split_index:]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.fit(X_train, y_train, epochs=1, batch_size=batch_size)\n",
    "        val_accuracy = model.evaluate(X_val, y_val)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Validation Accuracy: {val_accuracy}')\n",
    "        #Mini-batch gradient descent\n",
    "\n",
    "train_with_validation(model, X_train_subset, y_train_subset, epochs=10, batch_size=100, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "\n",
    "accuracy = model.evaluate(X_test, y_test_cat)\n",
    "print(f'Subset Training Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561f12b4",
   "metadata": {},
   "source": [
    "### **What we did:**\n",
    "    \n",
    "**Using a Smaller Subset for Initial Experiments:** \n",
    "\n",
    "We selected 10% of the training data to quickly iterate and experiment with different hyperparameters.\n",
    "\n",
    "**Experimenting with Initial Hidden Layer Size and Learning Rate:** \n",
    "\n",
    "We chose a hidden layer size of 100 and a learning rate of 0.17 to start our experiments.\n",
    "\n",
    "**Creating the Model:** \n",
    "\n",
    "We instantiated the model using the create_model function with the chosen hidden layer size and learning rate.\n",
    "\n",
    "**Training the Model with Validation:** \n",
    "\n",
    "We split the subset data into training and validation sets, trained the model for 10 epochs, and printed the validation accuracy after each epoch to monitor performance.\n",
    "\n",
    "**Evaluating the Model on the Test Set:** \n",
    "\n",
    "We evaluated the model on the test set to measure its accuracy and generalization ability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a51e4be",
   "metadata": {},
   "source": [
    "### What happens if we change the variables?\n",
    "\n",
    "**hidden_layer_size:** Different sizes can capture different levels of complexity in the data.\n",
    "\n",
    "**learning_rate:** Adjusting this affects how quickly the model converges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0fb1df",
   "metadata": {},
   "source": [
    "**Conclusions:** The model's validation accuracy improves steadily across the epochs, indicating that it is learning from the training data. Starting from 14.92% in the first epoch, it rises to 51.33% by the tenth epoch. However, the final accuracy on the test set is 48.62%, suggesting potential overfitting or that the model has not fully captured the underlying patterns in the dataset. This discrepancy highlights the need for further tuning or more complex models to improve generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f235764c",
   "metadata": {},
   "source": [
    "### Part D: Run the backpropagation algorithm on the entire dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea4c0a8",
   "metadata": {},
   "source": [
    "### **Train on the Entire Dataset**\n",
    "\n",
    "Train the model with the chosen parameters on the entire dataset.\n",
    "\n",
    "Evaluate the model on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a831f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training dataset:  (60000, 784)\n",
      "Epoch 1/10, Validation Accuracy: 0.9180833333333334\n",
      "Epoch 2/10, Validation Accuracy: 0.9369166666666666\n",
      "Epoch 3/10, Validation Accuracy: 0.9454166666666667\n",
      "Epoch 4/10, Validation Accuracy: 0.9524166666666667\n",
      "Epoch 5/10, Validation Accuracy: 0.954\n",
      "Epoch 6/10, Validation Accuracy: 0.9564166666666667\n",
      "Epoch 7/10, Validation Accuracy: 0.9606666666666667\n",
      "Epoch 8/10, Validation Accuracy: 0.9611666666666666\n",
      "Epoch 9/10, Validation Accuracy: 0.9645833333333333\n",
      "Epoch 10/10, Validation Accuracy: 0.9649166666666666\n",
      "Full Training Accuracy: 0.9673\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the training dataset: \", X_train.shape)\n",
    "\n",
    "hidden_layer_size = 50\n",
    "learning_rate = 0.17\n",
    "\n",
    "# Create the model with the parameters above\n",
    "model = create_model(hidden_layer_size, learning_rate)\n",
    "\n",
    "#The function to train the model has been specified in Part C\n",
    "train_with_validation(model, X_train, y_train_cat, epochs=10, batch_size=100, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "\n",
    "accuracy = model.evaluate(X_test, y_test_cat)\n",
    "print(f'Full Training Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ca55ce",
   "metadata": {},
   "source": [
    "**What we did:**\n",
    "    \n",
    "**Different Hidden Layer Size:** \n",
    "\n",
    "We chose a hidden layer size of 64 for fine-tuning to explore its impact on model performance.\n",
    "\n",
    "**Different Learning Rate:** \n",
    "\n",
    "We set the learning rate to 0.001 to fine-tune the model's convergence speed and stability.\n",
    "\n",
    "**Creating the Model with Fine-Tuned Parameters:** \n",
    "\n",
    "We instantiated the model using the create_model function with the specified hidden layer size and learning rate.\n",
    "\n",
    "**Training the Model on the Entire Dataset:** \n",
    "\n",
    "We trained the model using the full dataset with a validation split to monitor the performance on unseen data during training, running for 10 epochs with a batch size of 32.\n",
    "\n",
    "**Evaluating the Model on the Test Set:** \n",
    "\n",
    "We evaluated the model on the test set to measure its accuracy and generalization ability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c930888a",
   "metadata": {},
   "source": [
    "The model's validation accuracy demonstrates significant improvement over the epochs, starting at 48.43% in the first epoch and reaching 83.89% by the tenth epoch. This sharp increase, particularly from the fourth epoch onward, indicates that the model effectively learns and adapts to the training data. The final test accuracy of 83.26% suggests good generalization to unseen data. These results imply that the chosen hidden layer size and learning rate are well-suited for the task, successfully capturing the underlying patterns in the dataset. Continued tuning and potential architectural enhancements could further optimize performance and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3134b3b",
   "metadata": {},
   "source": [
    "### What happens if we change the variables?\n",
    "\n",
    "**hidden_layer_size:** Increasing the size can capture more complex patterns but may lead to overfitting. Decreasing it may simplify the model, potentially missing complex patterns.\n",
    "\n",
    "**learning_rate:** A higher learning rate speeds up convergence but risks overshooting optimal weights, while a lower rate improves stability but requires more iterations.\n",
    "\n",
    "**epochs:** More epochs allow the model to learn better but might lead to overfitting.\n",
    "\n",
    "**batch_size:** Smaller batch sizes provide a more accurate gradient estimate but increase training time, while larger batches are faster but may result in less accurate gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3caadfb",
   "metadata": {},
   "source": [
    "### Final Conclusions\n",
    "\n",
    "**Model Performance:**\n",
    "\n",
    "**Validation Accuracy:** Improved significantly over the epochs, indicating effective learning. Started at 48.43% in the first epoch and reached 83.89% by the tenth epoch.\n",
    "\n",
    "**Test Accuracy:** Final test accuracy of 83.26% suggests good generalization to unseen data.\n",
    "Hyperparameter Effectiveness:\n",
    "\n",
    "**Hidden Layer Size (64):** Effective in capturing necessary patterns without overfitting.\n",
    "\n",
    "**Learning Rate (0.001):** Balanced convergence speed and stability, leading to good performance.\n",
    "\n",
    "### Validation Insights:\n",
    "\n",
    "**Rapid Improvement:** Indicates the model's ability to adapt and learn effectively after initial epochs.\n",
    "\n",
    "**Good Generalization:** High final validation accuracy suggests the model effectively learned patterns in the dataset.\n",
    "Recommendations for Future Work\n",
    "\n",
    "**Model Selection:** Ensure that the chosen model architecture is well-suited to the data's complexity.\n",
    "\n",
    "**Feature Engineering:** Further exploration of additional features and their interactions can provide better insights and improvements.\n",
    "\n",
    "**Parameter Tuning:** Fine-tuning hyperparameters systematically can lead to optimized model performance.\n",
    "\n",
    "**Regularization:** Implementing regularization techniques can help mitigate overfitting in neural networks.\n",
    "\n",
    "**Cross-Validation:** Using cross-validation techniques can provide a more robust evaluation of model performance.\n",
    "\n",
    "**Ensemble Methods:** Exploring ensemble methods might improve performance by leveraging the strengths of different models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
