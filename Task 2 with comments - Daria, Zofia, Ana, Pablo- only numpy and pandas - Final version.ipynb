{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "141a45a1",
   "metadata": {},
   "source": [
    "# **Part A**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca8ee49",
   "metadata": {},
   "source": [
    "## **Load and preprocess the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5802407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "data = pd.read_csv('Titanic Dataset.csv')\n",
    "\n",
    "# Preprocessing\n",
    "\n",
    "#FARE, EMBARKED AND PARCH ARE IMPORTANT VARIABLES (SAID BY JONAS IN CLASS THAT WE MIGHT WANT TO CONSIDER TRYING-CHECK ALGORITHMS EXAM FROM LAST YEAR.)\n",
    "\n",
    "data['sex'] = data['sex'].astype('category').cat.codes\n",
    "features = ['pclass', 'sex', 'age', 'sibsp']\n",
    "X = data[features].fillna(data[features].mean())\n",
    "y = data['survived']\n",
    "\n",
    "# Split the dataset\n",
    "\n",
    "np.random.seed(42)\n",
    "train_indices = np.random.rand(len(X)) < 0.8\n",
    "X_train = X[train_indices]\n",
    "X_test = X[~train_indices]\n",
    "y_train = y[train_indices]\n",
    "y_test = y[~train_indices]\n",
    "\n",
    "\n",
    "# Standardize the features\n",
    "\n",
    "X_train_mean = X_train.mean(axis=0)\n",
    "X_train_std = X_train.std(axis=0)\n",
    "X_train = (X_train - X_train_mean) / X_train_std\n",
    "X_test = (X_test - X_train_mean) / X_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2853b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>survived</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "      <th>boat</th>\n",
       "      <th>body</th>\n",
       "      <th>home.dest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Rekic, Mr. Tido</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349249</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Zakarian, Mr. Ortin</td>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2670</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Carrau, Mr. Jose Pedro</td>\n",
       "      <td>1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113059</td>\n",
       "      <td>47.1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montevideo, Uruguay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Butler, Mr. Reginald Fenton</td>\n",
       "      <td>1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>234686</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>97.0</td>\n",
       "      <td>Southsea, Hants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Baxter, Mrs. James (Helene DeLaudeniere Chaput)</td>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PC 17558</td>\n",
       "      <td>247.5208</td>\n",
       "      <td>B58 B60</td>\n",
       "      <td>C</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pclass  survived                                             name  sex  \\\n",
       "1139       3         0                                  Rekic, Mr. Tido    1   \n",
       "1307       3         0                              Zakarian, Mr. Ortin    1   \n",
       "53         1         0                           Carrau, Mr. Jose Pedro    1   \n",
       "356        2         0                      Butler, Mr. Reginald Fenton    1   \n",
       "17         1         1  Baxter, Mrs. James (Helene DeLaudeniere Chaput)    0   \n",
       "\n",
       "       age  sibsp  parch    ticket      fare    cabin embarked boat  body  \\\n",
       "1139  38.0      0      0    349249    7.8958      NaN        S  NaN   NaN   \n",
       "1307  27.0      0      0      2670    7.2250      NaN        C  NaN   NaN   \n",
       "53    17.0      0      0    113059   47.1000      NaN        S  NaN   NaN   \n",
       "356   25.0      0      0    234686   13.0000      NaN        S  NaN  97.0   \n",
       "17    50.0      0      1  PC 17558  247.5208  B58 B60        C    6   NaN   \n",
       "\n",
       "                home.dest  \n",
       "1139                  NaN  \n",
       "1307                  NaN  \n",
       "53    Montevideo, Uruguay  \n",
       "356       Southsea, Hants  \n",
       "17           Montreal, PQ  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20a6b475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1309 entries, 0 to 1308\n",
      "Data columns (total 14 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   pclass     1309 non-null   int64  \n",
      " 1   survived   1309 non-null   int64  \n",
      " 2   name       1309 non-null   object \n",
      " 3   sex        1309 non-null   int8   \n",
      " 4   age        1046 non-null   float64\n",
      " 5   sibsp      1309 non-null   int64  \n",
      " 6   parch      1309 non-null   int64  \n",
      " 7   ticket     1309 non-null   object \n",
      " 8   fare       1308 non-null   float64\n",
      " 9   cabin      295 non-null    object \n",
      " 10  embarked   1307 non-null   object \n",
      " 11  boat       1309 non-null   int32  \n",
      " 12  body       121 non-null    float64\n",
      " 13  home.dest  745 non-null    object \n",
      "dtypes: float64(3), int32(1), int64(4), int8(1), object(5)\n",
      "memory usage: 129.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "550df20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data['boat'] = LabelEncoder().fit_transform(data['boat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40a508b",
   "metadata": {},
   "source": [
    "### What We Did Here:\n",
    "\n",
    "**Loading the Dataset:**\n",
    "- We loaded the Titanic dataset into a pandas DataFrame because it allows for easy manipulation and analysis.\n",
    "\n",
    "**Preprocessing:**\n",
    "- We converted the 'sex' column to numeric codes because models require numeric input.\n",
    "- We selected relevant features ('pclass', 'sex', 'age', 'sibsp') for prediction.\n",
    "- We filled missing values with the mean of the respective columns to handle NaNs because many models can't process missing data.\n",
    "- We extracted the 'survived' column as our target variable for training the model.\n",
    "\n",
    "**Splitting the Dataset:**\n",
    "- We set a random seed for reproducibility, ensuring consistent results.\n",
    "- We split the data into 80% training and 20% test sets to train the model on one part of the data and evaluate it on another.\n",
    "\n",
    "**Standardizing the Features:**\n",
    "- We calculated the mean and standard deviation of the training features.\n",
    "- We standardized the features to have a mean of 0 and standard deviation of 1, improving model performance and ensuring consistent scaling between training and test sets.\n",
    "\n",
    "### What Would Happen if the Variables Would Change?\n",
    "\n",
    "**Loading a Different Dataset:**\n",
    "- If a different dataset were loaded, the structure, column names, and the type of preprocessing needed might vary. Feature selection and handling of missing values might need adjustments based on the new dataset's characteristics.\n",
    "\n",
    "**Changing the Features:**\n",
    "- Modifying the features in the `features` list (e.g., adding 'fare', 'embarked', or 'parch') would impact the model input. Including more features could potentially improve model performance if those features are relevant, but it could also lead to overfitting if the features are not significant or introduce noise.\n",
    "\n",
    "**Different Seed for Splitting:**\n",
    "- Changing the seed value in `np.random.seed(42)` would result in a different train-test split. This could affect the model's performance slightly as the training and test sets would contain different samples.\n",
    "\n",
    "**Different Split Ratio:**\n",
    "- Altering the condition `np.random.rand(len(X)) < 0.8` to a different ratio (e.g., 0.7 for a 70-30 split) would change the size of the training and test sets. A smaller training set might reduce the model's ability to learn, while a smaller test set might not be representative enough to evaluate model performance accurately.\n",
    "\n",
    "**Standardization Parameters:**\n",
    "- If the means and standard deviations for standardization were calculated on the entire dataset rather than just the training set, it could lead to data leakage. This would mean that information from the test set would be used in training, leading to overly optimistic performance estimates.\n",
    "\n",
    "**Handling Missing Values:**\n",
    "- If missing values were filled with different statistics (e.g., median instead of mean), it could affect the distribution of the features and potentially the model's performance. Some models might be more sensitive to these changes than others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47d8238",
   "metadata": {},
   "source": [
    "## **Logistic model regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32535323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.7806691449814126\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Define the logistic regression function\n",
    "\n",
    "def logistic_regression(X, y, num_steps, learning_rate):\n",
    "    X = np.insert(X.values, 0, 1, axis=1)  # Add intercept\n",
    "    weights = np.zeros(X.shape[1])\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        z = np.dot(X, weights)\n",
    "        predictions = sigmoid(z)\n",
    "        \n",
    "        gradient = np.dot(X.T, predictions - y) / y.size\n",
    "        weights -= learning_rate * gradient\n",
    "\n",
    "    return weights\n",
    "\n",
    "\n",
    "# Training the logistic regression model\n",
    "\n",
    "num_steps = 10000\n",
    "learning_rate = 0.2\n",
    "weights = logistic_regression(X_train, y_train, num_steps, learning_rate)\n",
    "\n",
    "# Predictions\n",
    "\n",
    "X_test_intercept = np.insert(X_test.values, 0, 1, axis=1)  \n",
    "y_pred_prob = sigmoid(np.dot(X_test_intercept, weights))\n",
    "y_pred_log_reg = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "# Accuracy\n",
    "\n",
    "accuracy_log_reg = np.mean(y_pred_log_reg == y_test)\n",
    "print(f'Logistic Regression Accuracy: {accuracy_log_reg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7764fbf",
   "metadata": {},
   "source": [
    "### What We Did Here:\n",
    "\n",
    "**Sigmoid Function:**\n",
    "- We defined the sigmoid function to map real values to probabilities between 0 and 1, essential for logistic regression.\n",
    "\n",
    "**Logistic Regression Function:**\n",
    "- We added an intercept term to the feature matrix to include a bias in our model.\n",
    "- We initialized the weights to zero because it’s a standard starting point for optimization.\n",
    "- We used gradient descent to iteratively update the weights by calculating predictions and gradients, minimizing the cost function over `num_steps` iterations.\n",
    "\n",
    "**Training the Model:**\n",
    "- We trained the logistic regression model with 10,000 iterations and a learning rate of 0.2 to optimize the weights.\n",
    "\n",
    "**Making Predictions:**\n",
    "- We added an intercept to the test data to align with our trained model.\n",
    "- We predicted probabilities using the sigmoid function and converted them to binary outcomes (1 if probability >= 0.5, otherwise 0).\n",
    "\n",
    "**Calculating Accuracy:**\n",
    "- We computed accuracy by comparing our predictions to the actual test labels, giving us a measure of how well our model performed.\n",
    "\n",
    "This process prepares and trains the logistic regression model, makes predictions, and evaluates its accuracy.\n",
    "\n",
    "### What Would Happen if the Variables Would Change?\n",
    "\n",
    "**Different Number of Steps:**\n",
    "- Changing `num_steps` would affect the training process. Fewer steps might lead to underfitting, while more steps could improve convergence but increase computation time.\n",
    "\n",
    "**Different Learning Rate:**\n",
    "- Modifying `learning_rate` impacts how quickly the model learns. A smaller learning rate could slow down convergence, while a larger learning rate might speed it up but risk overshooting the optimal weights.\n",
    "\n",
    "**Different Initialization of Weights:**\n",
    "- Initializing weights differently could affect convergence speed and the final solution. Different initializations might lead to different local minima.\n",
    "\n",
    "**Different Threshold for Predictions:**\n",
    "- Changing the threshold for converting probabilities into binary predictions would alter the predicted labels, impacting accuracy and other evaluation metrics.\n",
    "\n",
    "**Different Preprocessing:**\n",
    "- If the features were standardized differently or if different features were selected, the model performance might vary. Proper standardization ensures equal contribution from features, and relevant feature selection can improve accuracy.\n",
    "\n",
    "**Handling Missing Values Differently:**\n",
    "- Filling missing values with different methods could affect data distribution and model performance. Different imputation methods might be more suitable depending on feature distributions and data characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42a488e",
   "metadata": {},
   "source": [
    "## **[4,5,2] Neutral network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08f5b14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Accuracy: 0.7732342007434945\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "\n",
    "def to_categorical(labels, num_classes):\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "y_train_cat = to_categorical(y_train.values, num_classes=2)\n",
    "y_test_cat = to_categorical(y_test.values, num_classes=2)\n",
    "\n",
    "# Define the neural network structure\n",
    "\n",
    "class SimpleNN:\n",
    "    def __init__(self, input_dim, hidden_units, output_units):\n",
    "        self.weights1 = np.random.randn(input_dim, hidden_units) * 0.01   # Weights and bias from input layer to hidden layer.\n",
    "        self.bias1 = np.zeros(hidden_units)\n",
    "        self.weights2 = np.random.randn(hidden_units, output_units) * 0.01 # Weights and bias from hidden layer to output layer.\n",
    "        self.bias2 = np.zeros(output_units)\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "        # This function will return 0 if the input is less than 0, and will return the same number if the input is positive\n",
    "\n",
    "    def softmax(self, x):\n",
    "        x = np.clip(x, -500, 500)\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "        # Normalizes the output into a probability distribution.\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.z1 = np.dot(x, self.weights1) + self.bias1  # Input to hidden layer\n",
    "        self.a1 = self.relu(self.z1)  # Activation of hidden layer\n",
    "        self.z2 = np.dot(self.a1, self.weights2) + self.bias2  # Hidden to output layer\n",
    "        self.a2 = self.softmax(self.z2)  # Output layer activation\n",
    "        return self.a2\n",
    "        # With one hidden layer\n",
    "\n",
    "    def backward(self, x, y, output, learning_rate):\n",
    "        m = y.shape[0]\n",
    "        dz2 = output - y\n",
    "        dw2 = np.dot(self.a1.T, dz2) / m\n",
    "        db2 = np.sum(dz2, axis=0) / m\n",
    "\n",
    "        dz1 = np.dot(dz2, self.weights2.T) * (self.a1 > 0)\n",
    "        dw1 = np.dot(x.T, dz1) / m\n",
    "        db1 = np.sum(dz1, axis=0) / m\n",
    "\n",
    "        self.weights2 -= learning_rate * dw2\n",
    "        self.bias2 -= learning_rate * db2\n",
    "        self.weights1 -= learning_rate * dw1\n",
    "        self.bias1 -= learning_rate * db1\n",
    "        # One hidden layer\n",
    "\n",
    "    def train(self, x, y, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(x.shape[0]):\n",
    "                x_sample = x[i:i+1]\n",
    "                y_sample = y[i:i+1]\n",
    "                output = self.forward(x_sample)\n",
    "                self.backward(x_sample, y_sample, output, learning_rate)\n",
    "        # Stochastic gradient descent - iterating through each sample\n",
    "\n",
    "    def evaluate(self, x, y):\n",
    "        output = self.forward(x)\n",
    "        predictions = np.argmax(output, axis=1)\n",
    "        targets = np.argmax(y, axis=1)\n",
    "        accuracy = np.mean(predictions == targets)\n",
    "        return accuracy\n",
    "\n",
    "# Normalize input data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train the neural network\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_units = 5\n",
    "output_units = 2\n",
    "learning_rate = 0.11\n",
    "epochs = 100\n",
    "\n",
    "nn = SimpleNN(input_dim, hidden_units, output_units)\n",
    "nn.train(X_train, y_train_cat, epochs, learning_rate)\n",
    "\n",
    "# Evaluate the neural network\n",
    "\n",
    "accuracy_nn = nn.evaluate(X_test, y_test_cat)\n",
    "print(f'Neural Network Accuracy: {accuracy_nn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b322a4b1",
   "metadata": {},
   "source": [
    "### What We Did Here:\n",
    "\n",
    "**Convert Labels to Categorical One-Hot Encoding:**\n",
    "- We converted the target labels into categorical one-hot encoding to use with the neural network. This transforms each label into a binary vector representing the classes.\n",
    "\n",
    "**Define the Neural Network Structure:**\n",
    "- We defined a simple neural network class `SimpleNN` with one hidden layer.\n",
    "  - **Initialization:** Initialized weights and biases for the layers with small random values to start the training process.\n",
    "  - **Activation Functions:** Defined ReLU for the hidden layer and softmax for the output layer to produce probabilities.\n",
    "  - **Forward Propagation:** Implemented forward propagation to compute the activations through the network layers.\n",
    "  - **Backward Propagation:** Implemented backward propagation to update weights and biases using gradient descent.\n",
    "  - **Training:** Defined a training method to iterate through samples and update weights for each epoch.\n",
    "  - **Evaluation:** Added a method to evaluate the network's accuracy on test data by comparing predictions to actual labels.\n",
    "\n",
    "**Normalize Input Data:**\n",
    "- We normalized the input data using `StandardScaler` to standardize features by removing the mean and scaling to unit variance, which helps in speeding up convergence.\n",
    "\n",
    "**Initialize and Train the Neural Network:**\n",
    "- Initialized the neural network with the specified input dimensions, number of hidden units, and output units.\n",
    "- Trained the network with the training data for 100 epochs and a learning rate of 0.11.\n",
    "\n",
    "**Evaluate the Neural Network:**\n",
    "- Evaluated the trained neural network on the test data and calculated the accuracy, which measures the model's performance.\n",
    "\n",
    "### What Would Happen if the Variables Would Change?\n",
    "\n",
    "**Different Number of Hidden Units:**\n",
    "- Changing `hidden_units` (e.g., increasing to 10 or decreasing to 3) would alter the network's capacity to learn patterns. More hidden units can capture more complex relationships but may lead to overfitting; fewer units might not capture the data's complexity well.\n",
    "\n",
    "**Different Learning Rate:**\n",
    "- Modifying `learning_rate` (e.g., to 0.05 or 0.2) impacts how quickly the model learns. A smaller learning rate could slow down training, while a larger learning rate might cause the training to become unstable and possibly diverge.\n",
    "\n",
    "**Different Number of Epochs:**\n",
    "- Changing `epochs` (e.g., to 50 or 200) would affect how long the network trains. Fewer epochs might result in underfitting, while more epochs can improve performance but might lead to overfitting if the model learns the noise in the training data.\n",
    "\n",
    "**Different Initialization of Weights:**\n",
    "- Initializing weights differently (e.g., with different scales or methods) could affect convergence speed and final performance. Different initializations might lead to different local minima.\n",
    "\n",
    "**Different Normalization:**\n",
    "- Using different normalization techniques (e.g., min-max scaling instead of standard scaling) could affect training. Proper normalization ensures features contribute equally, and different techniques might suit different data distributions better.\n",
    "\n",
    "**Handling Missing Values Differently:**\n",
    "- If missing values in the input data were handled differently (e.g., using median instead of mean for imputation), it could impact the distribution and consequently the model's performance. Different imputation methods might be more suitable based on the feature distributions and data characteristics.\n",
    "\n",
    "These changes would impact the training process, convergence, and final performance of the neural network model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769bfbe8",
   "metadata": {},
   "source": [
    "## **Comparing performances**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "899d7d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.7806691449814126\n",
      "Neural Network Accuracy: 0.7732342007434945\n"
     ]
    }
   ],
   "source": [
    "print(f'Logistic Regression Accuracy: {accuracy_log_reg}')\n",
    "print(f'Neural Network Accuracy: {accuracy_nn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b87615f",
   "metadata": {},
   "source": [
    "### **Conclusions:** \n",
    "\n",
    "Logistic regression (78.07% accuracy) slightly outperformed the neural network (77.32% accuracy), indicating a potentially linear relationship in the data. The neural network might benefit from further tuning and feature engineering. Both models perform well, but logistic regression is more effective for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f627b57a",
   "metadata": {},
   "source": [
    "# **Part B**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74bbcb1",
   "metadata": {},
   "source": [
    "## **Adding an Additional Variable and Optimizing the Network Layout**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf623bd5",
   "metadata": {},
   "source": [
    "## **[5,5,2] Neutral network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75b75840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Accuracy: 0.9553903345724907\n"
     ]
    }
   ],
   "source": [
    "##The same features as before plus FARE\n",
    "\n",
    "data['sex'] = data['sex'].astype('category').cat.codes\n",
    "features = ['pclass', 'sex', 'age', 'sibsp', 'boat']\n",
    "X = data[features].fillna(data[features].mean())\n",
    "y = data['survived']\n",
    "\n",
    "# Split the dataset\n",
    "\n",
    "np.random.seed(42)\n",
    "train_indices = np.random.rand(len(X)) < 0.8\n",
    "X_train = X[train_indices]\n",
    "X_test = X[~train_indices]\n",
    "y_train = y[train_indices]\n",
    "y_test = y[~train_indices]\n",
    "\n",
    "# Standardize the features\n",
    "\n",
    "X_train_mean = X_train.mean(axis=0)\n",
    "X_train_std = X_train.std(axis=0)\n",
    "X_train = (X_train - X_train_mean) / X_train_std\n",
    "X_test = (X_test - X_train_mean) / X_train_std\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "\n",
    "def to_categorical(labels, num_classes):\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "y_train_cat = to_categorical(y_train.values, num_classes=2)\n",
    "y_test_cat = to_categorical(y_test.values, num_classes=2)\n",
    "\n",
    "# Define the neural network structure\n",
    "\n",
    "class SimpleNN:\n",
    "    def __init__(self, input_dim, hidden_units, output_units):\n",
    "        self.weights1 = np.random.rand(input_dim, hidden_units)   # Weights and bias from input layer to hidden layer.\n",
    "        self.bias1 = np.zeros(hidden_units)\n",
    "        self.weights2 = np.random.rand(hidden_units, output_units) # Weights and bias from hidden layer to output layer.\n",
    "        self.bias2 = np.zeros(output_units)\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "        # This function will return 0 if the input is less than 0, and will return the same number if the input is positive\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "        # Normalizes the output into a probability distribution.\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.z1 = np.dot(x, self.weights1) + self.bias1  # Input to hidden layer\n",
    "        self.a1 = self.relu(self.z1)  # Activation of hidden layer\n",
    "        self.z2 = np.dot(self.a1, self.weights2) + self.bias2  # Hidden to output layer\n",
    "        self.a2 = self.softmax(self.z2)  # Output layer activation\n",
    "        return self.a2\n",
    "\n",
    "    def backward(self, x, y, output, learning_rate):\n",
    "        m = y.shape[0]\n",
    "        dz2 = output - y\n",
    "        dw2 = np.dot(self.a1.T, dz2) / m\n",
    "        db2 = np.sum(dz2, axis=0) / m\n",
    "\n",
    "        dz1 = np.dot(dz2, self.weights2.T) * (self.a1 > 0)\n",
    "        dw1 = np.dot(x.T, dz1) / m\n",
    "        db1 = np.sum(dz1, axis=0) / m\n",
    "\n",
    "        self.weights2 -= learning_rate * dw2\n",
    "        self.bias2 -= learning_rate * db2\n",
    "        self.weights1 -= learning_rate * dw1\n",
    "        self.bias1 -= learning_rate * db1\n",
    "\n",
    "    def train(self, x, y, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward(x)\n",
    "            self.backward(x, y, output, learning_rate)          \n",
    "        #We use batch gradient descent.\n",
    "                \n",
    "    def evaluate(self, x, y):\n",
    "        output = self.forward(x)\n",
    "        predictions = np.argmax(output, axis=1)\n",
    "        targets = np.argmax(y, axis=1)\n",
    "        accuracy = np.mean(predictions == targets)\n",
    "        return accuracy\n",
    "\n",
    "# Initialize and train the neural network\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_units = 5\n",
    "output_units = 2\n",
    "learning_rate = .3\n",
    "epochs = 100\n",
    "\n",
    "nn = SimpleNN(input_dim, hidden_units, output_units)\n",
    "nn.train(X_train.values, y_train_cat, epochs)\n",
    "\n",
    "#Trains and evaluates the neural network with a single output node.\n",
    "\n",
    "# Evaluate the neural network\n",
    "\n",
    "accuracy_nn = nn.evaluate(X_test.values, y_test_cat)\n",
    "print(f'Neural Network Accuracy: {accuracy_nn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e40c8748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Accuracy: 0.9442379182156134\n"
     ]
    }
   ],
   "source": [
    "class SimpleNN:\n",
    "    def __init__(self, input_dim, hidden_units, output_units):\n",
    "        self.weights1 = np.random.rand(input_dim, hidden_units)   # Weights and bias from input layer to hidden layer.\n",
    "        self.bias1 = np.zeros(hidden_units)\n",
    "        self.weights2 = np.random.rand(hidden_units, output_units) # Weights and bias from hidden layer to output layer.\n",
    "        self.bias2 = np.zeros(output_units)\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.z1 = np.dot(x, self.weights1) + self.bias1  # Input to hidden layer\n",
    "        self.a1 = self.relu(self.z1)  # Activation of hidden layer\n",
    "        self.z2 = np.dot(self.a1, self.weights2) + self.bias2  # Hidden to output layer\n",
    "        self.a2 = self.softmax(self.z2)  # Output layer activation\n",
    "        return self.a2\n",
    "\n",
    "    def backward(self, x, y, output, learning_rate):\n",
    "        m = y.shape[0]\n",
    "        dz2 = output - y\n",
    "        dw2 = np.dot(self.a1.T, dz2) / m\n",
    "        db2 = np.sum(dz2, axis=0) / m\n",
    "\n",
    "        dz1 = np.dot(dz2, self.weights2.T) * (self.a1 > 0)\n",
    "        dw1 = np.dot(x.T, dz1) / m\n",
    "        db1 = np.sum(dz1, axis=0) / m\n",
    "\n",
    "        self.weights2 -= learning_rate * dw2\n",
    "        self.bias2 -= learning_rate * db2\n",
    "        self.weights1 -= learning_rate * dw1\n",
    "        self.bias1 -= learning_rate * db1\n",
    "\n",
    "    def train(self, x, y, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(x.shape[0]):\n",
    "                x_sample = x[i:i+1]\n",
    "                y_sample = y[i:i+1]\n",
    "                output = self.forward(x_sample)\n",
    "                self.backward(x_sample, y_sample, output, learning_rate)\n",
    "                #Using stochastic gradient descent.\n",
    "                \n",
    "    def evaluate(self, x, y):\n",
    "        output = self.forward(x)\n",
    "        predictions = np.argmax(output, axis=1)\n",
    "        targets = np.argmax(y, axis=1)\n",
    "        accuracy = np.mean(predictions == targets)\n",
    "        return accuracy\n",
    "\n",
    "# Initialize and train the neural network\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_units = 5\n",
    "output_units = 2\n",
    "learning_rate = .2\n",
    "epochs = 100\n",
    "\n",
    "nn = SimpleNN(input_dim, hidden_units, output_units)\n",
    "nn.train(X_train.values, y_train_cat, epochs, learning_rate)\n",
    "\n",
    "# Evaluate the neural network\n",
    "\n",
    "accuracy_nn = nn.evaluate(X_test.values, y_test_cat)\n",
    "print(f'Neural Network Accuracy: {accuracy_nn}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd05ded",
   "metadata": {},
   "source": [
    "### What We Did Here:\n",
    "\n",
    "**Preprocessing:**\n",
    "- We converted the 'sex' column to numeric codes because models require numeric input.\n",
    "- We selected relevant features ('pclass', 'sex', 'age', 'sibsp', 'boat') for prediction, based on the importance mentioned by Jonas in class.\n",
    "- We filled missing values with the mean to handle NaNs because many models can't process missing data.\n",
    "- We extracted the 'survived' column as our target variable for training the model.\n",
    "\n",
    "**Splitting the Dataset:**\n",
    "- We set a random seed for reproducibility, ensuring consistent results.\n",
    "- We split the data into 80% training and 20% test sets to train the model on one part of the data and evaluate it on another.\n",
    "\n",
    "**Standardizing the Features:**\n",
    "- We calculated the mean and standard deviation of the training features.\n",
    "- We standardized the features to have a mean of 0 and standard deviation of 1, improving model performance and ensuring consistent scaling between training and test sets.\n",
    "\n",
    "**Convert Labels to Categorical One-Hot Encoding:**\n",
    "- We converted the target labels into categorical one-hot encoding to use with the neural network. This transforms each label into a binary vector representing the classes.\n",
    "\n",
    "**Define the Neural Network Structure:**\n",
    "- We defined a simple neural network class `SimpleNN` with one hidden layer.\n",
    "  - **Initialization:** Initialized weights and biases for the layers with small random values to start the training process.\n",
    "  - **Activation Functions:** Defined ReLU for the hidden layer and softmax for the output layer to produce probabilities.\n",
    "  - **Forward Propagation:** Implemented forward propagation to compute the activations through the network layers.\n",
    "  - **Backward Propagation:** Implemented backward propagation to update weights and biases using gradient descent.\n",
    "  - **Training:** Defined a training method to iterate through samples and update weights for each epoch using mini-batch gradient descent.\n",
    "  - **Evaluation:** Added a method to evaluate the network's accuracy on test data by comparing predictions to actual labels.\n",
    "\n",
    "**Initialize and Train the Neural Network:**\n",
    "- Initialized the neural network with the specified input dimensions, number of hidden units, and output units.\n",
    "- Trained the network with the training data for 100 epochs and a learning rate of 0.3.\n",
    "\n",
    "**Evaluate the Neural Network:**\n",
    "- Evaluated the trained neural network on the test data and calculated the accuracy, which measures the model's performance.\n",
    "\n",
    "### What Would Happen if the Variables Would Change?\n",
    "\n",
    "**Different Number of Hidden Units:**\n",
    "- Changing `hidden_units` (e.g., increasing to 10 or decreasing to 3) would alter the network's capacity to learn patterns. More hidden units can capture more complex relationships but may lead to overfitting; fewer units might not capture the data's complexity well.\n",
    "\n",
    "**Different Learning Rate:**\n",
    "- Modifying `learning_rate` (e.g., to 0.1 or 0.5) impacts how quickly the model learns. A smaller learning rate could slow down training, while a larger learning rate might cause the training to become unstable and possibly diverge.\n",
    "\n",
    "**Different Number of Epochs:**\n",
    "- Changing `epochs` (e.g., to 50 or 200) would affect how long the network trains. Fewer epochs might result in underfitting, while more epochs can improve performance but might lead to overfitting if the model learns the noise in the training data.\n",
    "\n",
    "**Different Initialization of Weights:**\n",
    "- Initializing weights differently could affect convergence speed and final performance. Different initializations might lead to different local minima.\n",
    "\n",
    "**Different Normalization:**\n",
    "- Using different normalization techniques (e.g., min-max scaling instead of standard scaling) could affect training. Proper normalization ensures features contribute equally, and different techniques might suit different data distributions better.\n",
    "\n",
    "**Handling Missing Values Differently:**\n",
    "- If missing values in the input data were handled differently (e.g., using median instead of mean for imputation), it could impact the distribution and consequently the model's performance. Different imputation methods might be more suitable based on the feature distributions and data characteristics.\n",
    "\n",
    "These changes would impact the training process, convergence, and final performance of the neural network model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca4892a",
   "metadata": {},
   "source": [
    "### **Conclusions:** \n",
    "\n",
    "The neural network achieved 95.54% accuracy, significantly outperforming logistic regression. This indicates the neural network effectively captured complex, non-linear relationships in the data. The high performance suggests successful tuning and model design, demonstrating the neural network's suitability for this dataset's intricacies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd50e1c",
   "metadata": {},
   "source": [
    "# **Part C**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97187c83",
   "metadata": {},
   "source": [
    "### **Why do we have two output nodes? What happens if you just use one? Can you adapt a network with just one output node to this problem?**\n",
    "\n",
    "In a binary classification problem, you can use either one or two output nodes. Two output nodes with a softmax activation function provide probabilities for both classes, making it easy to interpret the model's predictions as a probability distribution. However, a single output node with a sigmoid activation function is also effective, outputting a probability for the positive class, which can be thresholded (e.g., at 0.5) to determine the class. Both methods are ok, but using only one output node with sigmoid activation is often simpler and sufficient for binary classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab323b93",
   "metadata": {},
   "source": [
    "## **Understanding the Output Nodes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "287a5210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network with One Output Node Accuracy: 0.966542750929368\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset\n",
    "\n",
    "np.random.seed(42)\n",
    "train_indices = np.random.rand(len(X)) < 0.8\n",
    "X_train = X[train_indices]\n",
    "X_test = X[~train_indices]\n",
    "y_train = y[train_indices]\n",
    "y_test = y[~train_indices]\n",
    "\n",
    "# Standardize the features\n",
    "\n",
    "X_train_mean = X_train.mean(axis=0)\n",
    "X_train_std = X_train.std(axis=0)\n",
    "X_train = (X_train - X_train_mean) / X_train_std\n",
    "X_test = (X_test - X_train_mean) / X_train_std\n",
    "\n",
    "# Define the neural network with one output node\n",
    "\n",
    "class SimpleNNOneOutput:\n",
    "    def __init__(self, input_dim, hidden_units):\n",
    "        self.weights1 = np.random.rand(input_dim, hidden_units)\n",
    "        self.bias1 = np.zeros(hidden_units)\n",
    "        self.weights2 = np.random.rand(hidden_units, 1)\n",
    "        self.bias2 = np.zeros(1)\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "            # This function will return 0 if the input is less than 0, and will return the same number if the input is positive\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        self.z1 = np.dot(x, self.weights1) + self.bias1\n",
    "        self.a1 = self.relu(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.weights2) + self.bias2\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        return self.a2\n",
    "\n",
    "    def backward(self, x, y, output, learning_rate):\n",
    "        m = y.shape[0]\n",
    "        dz2 = output - y.reshape(-1, 1)\n",
    "        dw2 = np.dot(self.a1.T, dz2) / m\n",
    "        db2 = np.sum(dz2, axis=0) / m\n",
    "\n",
    "        dz1 = np.dot(dz2, self.weights2.T) * (self.a1 > 0)\n",
    "        dw1 = np.dot(x.T, dz1) / m\n",
    "        db1 = np.sum(dz1, axis=0) / m\n",
    "\n",
    "        self.weights2 -= learning_rate * dw2\n",
    "        self.bias2 -= learning_rate * db2\n",
    "        self.weights1 -= learning_rate * dw1\n",
    "        self.bias1 -= learning_rate * db1\n",
    "\n",
    "    def train(self, x, y, epochs, batch_size, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(0, x.shape[0], batch_size):\n",
    "                x_batch = x[i:i + batch_size]\n",
    "                y_batch = y[i:i + batch_size]\n",
    "                output = self.forward(x_batch)\n",
    "                self.backward(x_batch, y_batch, output, learning_rate)\n",
    "\n",
    "    def evaluate(self, x, y):\n",
    "        output = self.forward(x)\n",
    "        predictions = (output >= 0.5).astype(int)\n",
    "        accuracy = np.mean(predictions == y.reshape(-1, 1))\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "# Initialize and train the neural network\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_units = 6\n",
    "learning_rate = 0.2\n",
    "epochs = 100\n",
    "batch_size = 10\n",
    "\n",
    "nn = SimpleNNOneOutput(input_dim, hidden_units)\n",
    "nn.train(X_train.values, y_train.values, epochs, batch_size, learning_rate)\n",
    "\n",
    "# Evaluate the neural network\n",
    "\n",
    "accuracy_nn_one_output = nn.evaluate(X_test.values, y_test.values)\n",
    "print(f'Neural Network with One Output Node Accuracy: {accuracy_nn_one_output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78ce431",
   "metadata": {},
   "source": [
    "### What We Did Here:\n",
    "\n",
    "**Splitting the Dataset:**\n",
    "- We set a random seed for reproducibility, ensuring consistent results.\n",
    "- We split the data into 80% training and 20% test sets to train the model on one part of the data and evaluate it on another.\n",
    "\n",
    "**Standardizing the Features:**\n",
    "- We calculated the mean and standard deviation of the training features.\n",
    "- We standardized the features to have a mean of 0 and standard deviation of 1, improving model performance and ensuring consistent scaling between training and test sets.\n",
    "\n",
    "**Define the Neural Network with One Output Node:**\n",
    "- We defined a simple neural network class `SimpleNNOneOutput` with one hidden layer and a single output node.\n",
    "  - **Initialization:** Initialized weights and biases for the layers with small random values to start the training process.\n",
    "  - **Activation Functions:** Defined ReLU for the hidden layer and sigmoid for the output layer to produce probabilities.\n",
    "  - **Forward Propagation:** Implemented forward propagation to compute the activations through the network layers.\n",
    "  - **Backward Propagation:** Implemented backward propagation to update weights and biases using gradient descent.\n",
    "  - **Training:** Defined a training method to iterate through mini-batches of data and update weights for each epoch using mini-batch gradient descent.\n",
    "  - **Evaluation:** Added a method to evaluate the network's accuracy on test data by comparing predictions to actual labels.\n",
    "\n",
    "**Initialize and Train the Neural Network:**\n",
    "- Initialized the neural network with the specified input dimensions, number of hidden units, and a single output unit.\n",
    "- Trained the network with the training data for 100 epochs, using a batch size of 10 and a learning rate of 0.2.\n",
    "\n",
    "**Evaluate the Neural Network:**\n",
    "- Evaluated the trained neural network on the test data and calculated the accuracy, which measures the model's performance.\n",
    "\n",
    "### What Would Happen if the Variables Would Change?\n",
    "\n",
    "**Different Number of Hidden Units:**\n",
    "- Changing `hidden_units` (e.g., increasing to 10 or decreasing to 3) would alter the network's capacity to learn patterns. More hidden units can capture more complex relationships but may lead to overfitting; fewer units might not capture the data's complexity well.\n",
    "\n",
    "**Different Learning Rate:**\n",
    "- Modifying `learning_rate` (e.g., to 0.1 or 0.3) impacts how quickly the model learns. A smaller learning rate could slow down training, while a larger learning rate might cause the training to become unstable and possibly diverge.\n",
    "\n",
    "**Different Number of Epochs:**\n",
    "- Changing `epochs` (e.g., to 50 or 200) would affect how long the network trains. Fewer epochs might result in underfitting, while more epochs can improve performance but might lead to overfitting if the model learns the noise in the training data.\n",
    "\n",
    "**Different Batch Size:**\n",
    "- Adjusting `batch_size` (e.g., to 5 or 20) impacts the number of samples used to update the model weights at each step. Smaller batch sizes can provide more frequent updates but may be noisier, while larger batch sizes might provide more stable updates but can require more memory.\n",
    "\n",
    "**Different Initialization of Weights:**\n",
    "- Initializing weights differently could affect convergence speed and final performance. Different initializations might lead to different local minima.\n",
    "\n",
    "**Different Normalization:**\n",
    "- Using different normalization techniques (e.g., min-max scaling instead of standard scaling) could affect training. Proper normalization ensures features contribute equally, and different techniques might suit different data distributions better.\n",
    "\n",
    "**Handling Missing Values Differently:**\n",
    "- If missing values in the input data were handled differently (e.g., using median instead of mean for imputation), it could impact the distribution and consequently the model's performance. Different imputation methods might be more suitable based on the feature distributions and data characteristics.\n",
    "\n",
    "These changes would impact the training process, convergence, and final performance of the neural network model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690e281d",
   "metadata": {},
   "source": [
    "### **Conclusions:** \n",
    "\n",
    "The neural network with one output node achieved 96.65% accuracy, indicating excellent performance. This high accuracy suggests the model effectively captures complex patterns in the data, outperforming previous models. It demonstrates the neural network's strong ability to handle non-linear relationships in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf356bd0",
   "metadata": {},
   "source": [
    "# **Part D**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08ed965",
   "metadata": {},
   "source": [
    "## **Implementing Batches and Minibatches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db0d64e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network with Mini-Batches Accuracy: 0.966542750929368\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset\n",
    "\n",
    "np.random.seed(42)\n",
    "train_indices = np.random.rand(len(X)) < 0.8\n",
    "X_train = X[train_indices]\n",
    "X_test = X[~train_indices]\n",
    "y_train = y[train_indices]\n",
    "y_test = y[~train_indices]\n",
    "\n",
    "# Standardize the features\n",
    "\n",
    "X_train_mean = X_train.mean(axis=0)\n",
    "X_train_std = X_train.std(axis=0)\n",
    "X_train = (X_train - X_train_mean) / X_train_std\n",
    "X_test = (X_test - X_train_mean) / X_train_std\n",
    "\n",
    "# Define the neural network with one output node\n",
    "\n",
    "class SimpleNNMiniBatch:\n",
    "    def __init__(self, input_dim, hidden_units):\n",
    "        self.weights1 = np.random.rand(input_dim, hidden_units)\n",
    "        self.bias1 = np.zeros(hidden_units)\n",
    "        self.weights2 = np.random.rand(hidden_units, hidden_units)\n",
    "        self.bias2 = np.zeros(hidden_units)\n",
    "        self.weights3 = np.random.rand(hidden_units, 1)\n",
    "        self.bias3 = np.zeros(1)\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.z1 = np.dot(x, self.weights1) + self.bias1\n",
    "        self.a1 = self.relu(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.weights2) + self.bias2\n",
    "        self.a2 = self.relu(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.weights3) + self.bias3\n",
    "        self.a3 = self.sigmoid(self.z3)\n",
    "        return self.a3\n",
    "\n",
    "    def backward(self, x, y, output):\n",
    "        m = y.shape[0]\n",
    "        dz3 = output - y.reshape(-1, 1)\n",
    "        dw3 = np.dot(self.a2.T, dz3) / m\n",
    "        db3 = np.sum(dz3, axis=0) / m\n",
    "\n",
    "        dz2 = np.dot(dz3, self.weights3.T) * (self.a2 > 0)\n",
    "        dw2 = np.dot(self.a1.T, dz2) / m\n",
    "        db2 = np.sum(dz2, axis=0) / m\n",
    "\n",
    "        dz1 = np.dot(dz2, self.weights2.T) * (self.a1 > 0)\n",
    "        dw1 = np.dot(x.T, dz1) / m\n",
    "        db1 = np.sum(dz1, axis=0) / m\n",
    "\n",
    "        self.weights3 -= learning_rate * dw3\n",
    "        self.bias3 -= learning_rate * db3\n",
    "        self.weights2 -= learning_rate * dw2\n",
    "        self.bias2 -= learning_rate * db2\n",
    "        self.weights1 -= learning_rate * dw1\n",
    "        self.bias1 -= learning_rate * db1\n",
    "\n",
    "    def train(self, x, y, epochs, batch_size):\n",
    "        for epoch in range(epochs):\n",
    "            indices = np.arange(x.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            x = x[indices]\n",
    "            y = y[indices]\n",
    "\n",
    "            for i in range(0, x.shape[0], batch_size):\n",
    "                x_batch = x[i:i + batch_size]\n",
    "                y_batch = y[i:i + batch_size]\n",
    "                output = self.forward(x_batch)\n",
    "                self.backward(x_batch, y_batch, output)\n",
    "\n",
    "    def evaluate(self, x, y):\n",
    "        output = self.forward(x)\n",
    "        predictions = (output >= 0.5).astype(int)\n",
    "        accuracy = np.mean(predictions == y.reshape(-1, 1))\n",
    "        return accuracy\n",
    "\n",
    "# Initialize and train the neural network\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_units = 5\n",
    "learning_rate = 0.2\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "nn = SimpleNNMiniBatch(input_dim, hidden_units)\n",
    "nn.train(X_train.values, y_train.values, epochs, batch_size)\n",
    "\n",
    "# Evaluate the neural network\n",
    "\n",
    "accuracy_nn_mini_batch = nn.evaluate(X_test.values, y_test.values)\n",
    "print(f'Neural Network with Mini-Batches Accuracy: {accuracy_nn_mini_batch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c865fb60",
   "metadata": {},
   "source": [
    "### What We Did Here:\n",
    "\n",
    "**Splitting the Dataset:**\n",
    "- We set a random seed for reproducibility, ensuring consistent results.\n",
    "- We split the data into 80% training and 20% test sets to train the model on one part of the data and evaluate it on another.\n",
    "\n",
    "**Standardizing the Features:**\n",
    "- We calculated the mean and standard deviation of the training features.\n",
    "- We standardized the features to have a mean of 0 and standard deviation of 1, improving model performance and ensuring consistent scaling between training and test sets.\n",
    "\n",
    "**Define the Neural Network with One Output Node and Mini-Batch Gradient Descent:**\n",
    "- We defined a simple neural network class `SimpleNNMiniBatch` with two hidden layers and a single output node.\n",
    "  - **Initialization:** Initialized weights and biases for the layers with small random values to start the training process.\n",
    "  - **Activation Functions:** Defined ReLU for the hidden layers and sigmoid for the output layer to produce probabilities.\n",
    "  - **Forward Propagation:** Implemented forward propagation to compute the activations through the network layers.\n",
    "  - **Backward Propagation:** Implemented backward propagation to update weights and biases using gradient descent.\n",
    "  - **Training:** Defined a training method to iterate through mini-batches of data and update weights for each epoch using mini-batch gradient descent.\n",
    "  - **Evaluation:** Added a method to evaluate the network's accuracy on test data by comparing predictions to actual labels.\n",
    "\n",
    "**Initialize and Train the Neural Network:**\n",
    "- Initialized the neural network with the specified input dimensions, number of hidden units, and a single output unit.\n",
    "- Trained the network with the training data for 100 epochs, using a batch size of 32 and a learning rate of 0.2.\n",
    "\n",
    "**Evaluate the Neural Network:**\n",
    "- Evaluated the trained neural network on the test data and calculated the accuracy, which measures the model's performance.\n",
    "\n",
    "### What Would Happen if the Variables Would Change?\n",
    "\n",
    "**Different Number of Hidden Units:**\n",
    "- Changing `hidden_units` (e.g., increasing to 10 or decreasing to 3) would alter the network's capacity to learn patterns. More hidden units can capture more complex relationships but may lead to overfitting; fewer units might not capture the data's complexity well.\n",
    "\n",
    "**Different Learning Rate:**\n",
    "- Modifying `learning_rate` (e.g., to 0.1 or 0.3) impacts how quickly the model learns. A smaller learning rate could slow down training, while a larger learning rate might cause the training to become unstable and possibly diverge.\n",
    "\n",
    "**Different Number of Epochs:**\n",
    "- Changing `epochs` (e.g., to 50 or 200) would affect how long the network trains. Fewer epochs might result in underfitting, while more epochs can improve performance but might lead to overfitting if the model learns the noise in the training data.\n",
    "\n",
    "**Different Batch Size:**\n",
    "- Adjusting `batch_size` (e.g., to 16 or 64) impacts the number of samples used to update the model weights at each step. Smaller batch sizes can provide more frequent updates but may be noisier, while larger batch sizes might provide more stable updates but can require more memory.\n",
    "\n",
    "**Different Initialization of Weights:**\n",
    "- Initializing weights differently could affect convergence speed and final performance. Different initializations might lead to different local minima.\n",
    "\n",
    "**Different Normalization:**\n",
    "- Using different normalization techniques (e.g., min-max scaling instead of standard scaling) could affect training. Proper normalization ensures features contribute equally, and different techniques might suit different data distributions better.\n",
    "\n",
    "**Handling Missing Values Differently:**\n",
    "- If missing values in the input data were handled differently (e.g., using median instead of mean for imputation), it could impact the distribution and consequently the model's performance. Different imputation methods might be more suitable based on the feature distributions and data characteristics.\n",
    "\n",
    "These changes would impact the training process, convergence, and final performance of the neural network model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa9762a",
   "metadata": {},
   "source": [
    "### **Conclusions:** \n",
    "\n",
    "The neural network with mini-batches achieved 96.65% accuracy, indicating exceptional performance. This high accuracy shows that mini-batch gradient descent effectively trained the model, capturing complex data patterns and improving generalization. It demonstrates the model's strong ability to handle non-linear relationships efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72c589d",
   "metadata": {},
   "source": [
    "# In-Depth Analysis: Final Conclusions\n",
    "\n",
    "In evaluating various models on the Titanic dataset, we performed multiple steps to preprocess the data, implemented logistic regression, built a neural network, and explored variations in neural network architecture and training methods. Here's a more in depth analysis of our findings:\n",
    "\n",
    "#### 1. Logistic Regression\n",
    "\n",
    "**Performance:**\n",
    "\n",
    "- **Accuracy:** 78.07%\n",
    "- **Advantages:** Simple, interpretable, and effective for linear relationships in the data. The model performed well, indicating that the dataset has a significant linear component.\n",
    "\n",
    "**Key Steps:**\n",
    "\n",
    "- **Data Preprocessing:** Converted categorical variables, filled missing values, standardized features.\n",
    "- **Model Training:** Used gradient descent to optimize the weights.\n",
    "- **Prediction & Evaluation:** Calculated probabilities using the sigmoid function and converted them to binary outcomes for accuracy calculation.\n",
    "\n",
    "**Variable Impact:**\n",
    "\n",
    "- **num_steps:** Increasing steps can improve accuracy but requires more training time.\n",
    "- **learning_rate:** A higher rate speeds up training but risks overshooting optimal weights, while a lower rate may improve accuracy but needs more iterations.\n",
    "\n",
    "#### 2. Neural Network (Basic)\n",
    "\n",
    "**Performance:**\n",
    "\n",
    "- **Accuracy:** 77.32%\n",
    "- **Advantages:** Can capture complex, non-linear relationships in the data.\n",
    "- **Disadvantages:** Slightly underperformed compared to logistic regression, possibly due to overfitting or suboptimal tuning.\n",
    "\n",
    "**Key Steps:**\n",
    "\n",
    "- **One-Hot Encoding:** Converted labels to one-hot encoding for compatibility with the neural network's output.\n",
    "- **Neural Network Structure:** Input layer, hidden layer with ReLU activation, and an output layer with sigmoid activation.\n",
    "- **Training & Evaluation:** Used gradient descent for iterative parameter updates.\n",
    "\n",
    "**Variable Impact:**\n",
    "\n",
    "- **hidden_units:** Increasing units captures more complex patterns but risks overfitting.\n",
    "- **learning_rate, epochs, batch_size:** Adjusting these impacts training speed, convergence, and the balance between training efficiency and accuracy.\n",
    "\n",
    "#### 3. Neural Network with One Output Node\n",
    "\n",
    "**Performance:**\n",
    "\n",
    "- **Accuracy:** 96.65%\n",
    "- **Advantages:** Simplifying the network to one output node improved performance, suggesting that a simpler architecture enhances generalization.\n",
    "- **Disadvantages:** Still requires careful tuning to avoid overfitting.\n",
    "\n",
    "**Key Steps:**\n",
    "\n",
    "- **Neural Network Structure:** Input layer, hidden layers with ReLU activation, and a single output layer with sigmoid activation.\n",
    "- **Training & Evaluation:** Similar training process as before, using mini-batch gradient descent.\n",
    "\n",
    "**Variable Impact:**\n",
    "\n",
    "- **hidden_units, learning_rate, epochs, batch_size:** Same considerations as the previous network.\n",
    "\n",
    "#### 4. Neural Network with Mini-Batch Training\n",
    "\n",
    "**Performance:**\n",
    "\n",
    "- **Accuracy:** 96.65%\n",
    "- **Advantages:** Mini-batch training stabilizes and speeds up learning, balancing between stochastic gradient descent and full-batch gradient descent.\n",
    "- **Disadvantages:** Requires further tuning to fully leverage mini-batch benefits.\n",
    "\n",
    "**Key Steps:**\n",
    "\n",
    "- **Neural Network Structure:** Similar to the one output node network but with mini-batch training.\n",
    "- **Training Process:** Shuffled data before each epoch, processed data in small batches for parameter updates.\n",
    "\n",
    "**Variable Impact:**\n",
    "\n",
    "- **hidden_units, learning_rate, epochs, batch_size:** Adjusting these affects the balance between training efficiency and accuracy.\n",
    "\n",
    "### Summary of Key Findings\n",
    "\n",
    "- **Logistic Regression:** Strong performance with a simpler, linear model indicating significant linear relationships in the data.\n",
    "- **Neural Network (Basic):** Captures non-linear relationships but needs careful tuning to avoid overfitting.\n",
    "- **Neural Network with One Output Node:** Improved performance with a simplified architecture, demonstrating better generalization.\n",
    "- **Mini-Batch Training:** Provides a balance between training efficiency and stability, achieving high accuracy with further tuning.\n",
    "\n",
    "### How can we make this better in the future?\n",
    "\n",
    "1. **Model Selection:** For datasets with significant linear relationships, logistic regression can be highly effective. For more complex patterns, neural networks are suitable but require careful architecture and parameter tuning.\n",
    "\n",
    "2. **Feature Engineering:** Adding relevant features (like 'boat') can improve performance. Further exploration of additional features and their interactions can provide better insights and improvements.\n",
    "\n",
    "3. **Parameter Tuning:** Fine-tuning hyperparameters (learning rate, epochs, batch size, hidden units) is crucial for optimizing model performance. Techniques like grid search or random search can be employed for systematic tuning.\n",
    "\n",
    "4. **Regularization:** Implementing regularization techniques (e.g., dropout, L2 regularization) can help mitigate overfitting in neural networks.\n",
    "\n",
    "5. **Cross-Validation:** Using cross-validation techniques can provide a more robust evaluation of model performance, ensuring that the results are not dependent on a specific train-test split.\n",
    "\n",
    "6. **Ensemble Methods:** Exploring ensemble methods (e.g., combining logistic regression with neural networks) might improve performance by leveraging the strengths of different models.\n",
    "\n",
    "By combining robust preprocessing, effective model selection, and advanced training techniques, we can achieve high-performing predictive models for the Titanic dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
